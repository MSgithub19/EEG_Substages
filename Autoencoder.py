import tensorflow
from tensorflow import keras
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pandas as pd

filepath= r"D:\Milosz_2023\Spectrograms\NN6\D8\Channel 12\Free Moving\NN6 FM D8 T7.csv"

'''Reading spectrograms in with pandas. 
Alternatively we could proceed directly with the spectrograms generated by generate.spectrogram()'''
spectrogram= pd.read_csv(filepath).values
'''Transpose the spectrogram matrix so frequency = columns'''
spectrogram = spectrogram.T

print(type(spectrogram))

'''Scaling the data just as we would for PCA
StandardScaler() or MinMaxScaler() depending on if we want to just normalise or scale between 0 and 1'''
scaling= StandardScaler()
scaled_data= scaling.fit_transform(spectrogram)

'''Checking the range of the scaled data to inform how we rate the MSE score'''
print("Min values per feature:\n", scaled_data.min(axis=0))
print("Max values per feature:\n", scaled_data.max(axis=0))


'''Defining the dimensions of the encoded/latent space representation
This seems to represent the number of columns'''
encoding_dim= 3000

'''Defining the input layer
if scaled_data had the shape (100,50) that would be 100 samples with 50 values or features
Therefore we set shape[1] to get the number of features for only 1 sample'''
input_data= Input(shape=(scaled_data.shape[1],))

'''Defining the first encoder layer
This is highly customisable
The function Dense() is for creating a fully connected neural network layer
The first argument indicates the number of "neurons" in that layer. Typically, more is better but it can lead to overfitting. Start with 128
The parameter activation specifies the activation function and relu stands for Rectified Linear Unit'''
encoded= Dense(64, activation='relu')(input_data)

'''Second encoder layer
It is once again dense, meaning fully connected
It takes the previously defined encoding_dimensions, and reduces the data to that shape
It also takes the previous encoded variable and uses it as the input, then overrides it with another encoded variable which is further compressed'''
encoded= Dense(encoding_dim, activation='relu')(encoded)

'''Defining the decoder layers
Much the same parameters here, the difference is that we are now taking the shape of one of our samples and decoding the latent space representation to fit that shape
The activation parameter, sigmoid, transforms the ouputs to be between 1 and 0, which should make it the same as our scaled input data if we used MinMax'''
decoded = Dense(64, activation='relu')(encoded)
#decoded = Dense(scaled_data.shape[1], activation='sigmoid')(decoded)
decoded = Dense(scaled_data.shape[1])(decoded)

'''Creating a model based on the input data and the decoded data'''
autoencoder= Model(input_data, decoded)

'''Preparing the autoencoder for training
the optimizer, Adam, is an example. Further investigation needed to determine the optimal optimizer, hehe
the loss function is also specified, in this case mean squared error. Again this can be customised but it seems like a reasonable choice for the purpose of this autoencoder'''
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

'''Training the autoencoder
The first two inputs to fit() represent the input and target data, which in this case is both the scaled_data
The autoencoder is trained for x epochs with a batch size of y, to be adjusted, and the data is shuffled every between every epoch
I have also added a stop function here which will stop the training if the validation scores do not improve within 100 epochs'''
early_stopping = EarlyStopping(monitor='val_loss', patience=300)

autoencoder.fit(scaled_data, scaled_data,
                epochs=1000,
                batch_size=400,
                shuffle=True,
                validation_split=0.2,  # Using 20% of the data for validation
                callbacks=[early_stopping])


'''Creating a model for the encoder
takes the input data and outputs the encoded data'''
encoder = Model(input_data, encoded)

'''Generating encoded data from the input'''
encoded_data = encoder.predict(scaled_data)

''''Evaluating data reconstruction using mean squared error (MSE)
A lower MSE is better in this context'''
reconstructed_data = autoencoder.predict(scaled_data)
mse = np.mean(np.square(scaled_data - reconstructed_data))
print("Mean Squared Error:", mse)

'''The encoded data is analogous to a PCA result, so lets save it for analysis in RStudio
This data is already transposed and scaled, so these steps are not necessary in RStudio'''
encoded_data_df = pd.DataFrame(encoded_data)
encoded_data_df.to_csv(r"D:\Milosz_2023\Spectrograms\NN6\Autoencoder data\encode_NN6 FM D8 T7.csv", index=False, header=False)
